hystrix 存在的缺点
1需要程序员手动搭建监控平台
2没有一套web界面可以给我们进行更加细粒度的配置流量控制、速率控制、服务熔断、服务降级

sentinel优点
1单独一个组件，可以独立出来
2直接界面化细粒度统一配置
约定>配置>编码
建议使用配置+注解的形式

seata分布式事务
下订单--->支付--->减库存--->金额扣减
多服务和多个数据源统一调度问题
每个服务内部的数据一致性仅能够由本地事务保证
全局数据一致性保证问题
springcloud alibaba的分布式事务解决组件技术Seata
组成：
    一个ID+3个组件
	Transaction ID全局唯一事务ID，保证全局唯一ID的机制？见下方答案
	Transaction Coordinator(TC)事务协调者，维护全局事务的状态，负责协调并驱动全局事务的提交和回滚
	Transaction Manager(TM)事务管理者，控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议,事务的发起者
	Resource Manager(RM)资源管理者，控制分支事务，负责分支注册、状态汇报，并接受TC的指令，驱动本地分支事务的提交和回滚,事务的参与者

过程：
   1TM向TC申请一个全局事务，创建成功后会生成一个全局唯一的XID
   2XID在微服务调用链的上下文中传播
   3RM向TC中注册分支事务，将其纳入XID对应全局事务的管辖
   4TM向TC发起针对XID的全局提交或回滚决议
   5TC调度XID下的全局分支事务完成提交或者回滚

全局@GlobalTransactional

配置步骤
	1找到conf文件夹下面的file.conf中的service修改groupid，修改store的类型为db，且修改数据库配置文件
	2将同级文件db_store.sql的数据库脚本在Mysql的seata数据库中执行
	3修改registry.conf配置文件，将注册中心改为nacos

用户下单，会先在订单服务中创建一个订单，然后远程调用库存服务来扣减下单商品的库存
再远程调用账户服务来扣减用户账户的余额，
最后在订单服务中修改订单的状态，标记已完成。
三个数据库，两次RPC远程调用

seata原理剖析
2019蚂蚁金服和阿里巴巴的开源分布式事务解决方案
2020年后使用1.0.0+版本,0.9.0无法使用集群	
TM开启分布式事务（TM向TC注册全局事务记录）
按照业务场景，编排数据库、服务等事务内资源（RM向TC汇报资源准备状态）
TM结束分布式事务，事务一阶段结束（TM通知TC提交/回滚分布式事务）
TM汇总所有的事务信息，决定分布式事务是提交还是回滚
TC通知所有的RM提交/回滚资源，事务二阶段结束


提供了AT、TCC、SAGA（长事务）和XA模式，
默认为AT模式（低侵入，自动补偿）
两阶段提交协议
	一阶段：业务数据t_order和回滚日志记录undo_log在同一个本地事务中提交，释放本地锁和连接资源
	  -分析：在一阶段Seata会拦截业务SQL。
	        (1)解析SQL语义，找到业务SQL要更新的业务数据，在业务数据更新之前，将其保存为before image（前置镜像SQL）
			(2)执行业务SQL更新业务数据
			(3)在业务数据更新之后，将其保存为after image（后置镜像SQL），最后生成行级锁
			上述两步在一个数据库事务内完成，保证了原子性。
	二阶段：提交异步化,非常快速的完成
			回滚通过一阶段的回滚日志进行反向补偿
	   -分析二阶段提交:
			业务SQL已经在一阶段提交至数据库，因此Seata只需要将一阶段保存的快照数据（before image和after image）、
			行级锁和undo_log日志删除，完成数据清理。
	   -分析二阶段回滚:
			Seata需要回滚一阶段已执行的业务SQL，还原出业务数据。用before image还原业务数据，首先校验脏写，
			对比数据库当前业务数据和after image，如果两份数据完全一样就说明没有脏写，可以还原业务数据，如
			果不一致就说明有脏写，出现脏写此时需要人工处理，若不出现脏写，则完成数据还原后删除before image
			、after image、行级锁和undo_log日志。
debug
	branch_table：xid为全局事务id=IP地址+端口号+随机数,RM注册到TC的所有记录均在此表，记录数与调用的服务个数有关，
				  branchid为每个服务本地的事务id，rollback_info保存了before image和after image


高频面试问题
(1)集群高并发的情况下，如何保证分布式唯一全局id的生成
   集群中需要对大量的数据和消息进行唯一标识，如电商订单的id
   id生成规则的要求
			1全局唯一：不能出现重复的id号码
			2趋势递增：Mysql的InnoDB引擎使用Btree的结构存储索引数据，使用有序的数据保证写入的性能
			3单调递增：尽量保证下一个id比上一个id大
			4信息安全：若ID连续容易暴露信息给黑客，需要id无规则
			5含时间戳：能够快速了解id生成的时间
	id生成系统的可用性要求：
			1高可用: 99.999%的稳定性
			2低延迟：毫米级
			3高Qps: 百万级响应
	全局唯一ID解决方案为：
			1UUID:32位的16进制字符串8-4-4-4-12，虽不耗网路资源，但入库性能差，不建议生产部署
				 (1)无法预估id的规律
				 (2)mysql建议主键小因为索引会占用较大空间
				 (3)InnoDB的BTree插入UUID时，因为UUID无序，会产生较多的分裂不饱和节点，降低数据库的插入性能
			2数据库自增逐渐
				 (1)单机可以
				 (2)通过mysql的自增id和replace into实现，replace into类似insert into，尝试插入数据列表，若数据已经存在则先删除，
					再插入，否则直接插入。
					系统水平扩展困难，当定义好步长和机器数之后，添加新的机器无法调整。
					数据库压力大，每次获取ID需读写一次数据库，影响性能，不满足高QPS。
					不适用于集群。
			3基于Redis生成全局id策略
				 Redis6.0已开始支持多线程
				 Redis目前是单线程，天生保证原子性，可以使用INCR和INCRBY实现
				 Redis集群步长需要设置，过期时间需要设置，满足高QPS的需求
				 假如有5台Redis服务器，分别初始化Redis从1，2,3,4,5开始，步长均为5
				 则各个Redis的ID分别为
				 A: 1,6,11,16,21
				 B: 2,7,12,17,22
				 C: 3,8,13,18,23
				 D: 4,9,14,19,24
				 E: 5,10,15,20,25
			  缺点：配置维护工作较多，设置哨兵模式防止单点故障
	snowflake: 雪花算法（Twitter的）
		背景：Twitter迁移数据从Mysql到Cassandra（无ID顺序生成机制），开发了一套全局唯一ID生成服务
		性能：1每秒26万个自增可排序的ID
			  2生成的ID能够按照时间有序生成
			  3生成的id为一个64bit的整数，为一个Long型（转换为字符串长度最多是19）
			  4不会产生ID碰撞（由datacenter和workerId作区分），且效率高。